{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "List of some todos for the paper..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerated Basic Algorithm\n",
    "\n",
    "1. [x] Benchmark properly current version (`e8b51ef01e034f600a0e5b91174abe65a0ce5127`)\n",
    "    - `numba1` results\n",
    "2. [x] Adapt to recycle slots in the numpy jet arrays\n",
    "    1. Add a jet_index array, that maps a numpy slot to the Python PseudoJets structure, so that indexes no longer have to be in sync\n",
    "    2. Reduce the size of the numpy arrays as these will now not need spare slots at the end for merged jets\n",
    "        - could even experiment with repacking to ensure always dense packing of the numpy arrays\n",
    "\n",
    "This turned out to have far less impact that I was expecting - 10-15% reduction in runtime only\n",
    "- `numba2` results\n",
    "\n",
    "Profiling picked up that the `deepcopy` of the initial particles was having a very large impact (like 30% of runtime!). I adapted the code to avoid doing this in the timed loop\n",
    "- `numba3` results\n",
    "\n",
    "Realised that the `BasicJetInfo` class was not not needed - all of this state is tracked via the numpy arrays now. Removed that and got another speed up (~15%)\n",
    "- `numba4` results\n",
    "\n",
    "Aside - at this point I also commented out the `@njit` statements, just to understand what difference that would make. Looks like `numba` jitting is gaining around 40-50%.\n",
    "\n",
    "Commenting out `add_step_to_history` code gives another speed-up of about 15%, so actually the Python code that's left is now a significant drag on overall performance! But need to reimplement something else to get around that, as this actually stores the results.\n",
    "- Done (branch `history-numba`)\n",
    "- In fact speed up is very small ~2%!\n",
    "\n",
    "Discovered that removing all setter/getter code from `PseudoJet` is a but faster, so did that too\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerated Tiled Algorithm\n",
    "\n",
    "Need to have a think about to do this, but probably it needs to be a numpy array per-tile. This will require quite a lot more book-keeping, both to store the correct tiled structures and to keep track of the global state.\n",
    "\n",
    "How about a 3D array: [tile-phi][tile-rap][VAR]?\n",
    "\n",
    "Initial implementation is tricky - it works, but it's very dodgy trying to ensure that all jets get updated correctly as mergers happen\n",
    "- If we do a full update each cycle things are working, but it's very, very slow to do that\n",
    "- We do not have bi-directional tests ATM, so only the target pseudojet is being updated correctly\n",
    "    - Consequently we have to push far too many tiles into the update loop (basically +8, for all of the neighbouring tiles, meaning that 64 extra tile-tile comparisons are getting done each iteration!)\n",
    "- Another consequence of the uni-directional updates is that the rightward march when doing a full update doesn't work, so each tile needs to scan all of its neighbours (8 scans, instead of 4)\n",
    "\n",
    "Fixed this all by converting the `nn` array to hold a single flattened index with the nearest neighbour (the previous incarnation with a tuple was a bit of a mess - in fact then nn became a 4D array). This made the detection of jets/tiles where an update was required very much easier - it now works.\n",
    "\n",
    "Results validated against FastJet now!\n",
    "\n",
    "* Windows: Mean time per event 151,680.00 ± 6,268.43 us\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Philippe's Implementation\n",
    "\n",
    "Track down the bug that is causing differences with FastJet for a few of the sample events\n",
    "- event 32\n",
    "- event 53"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, seeing minor differences in the numerical values of the antikt distance metric, e.g., for event 32:\n",
    "\n",
    "```\n",
    "py: Iteration 13: 0.0015722253198696117 for jet 122 and jet 181\n",
    "jl: Iteration 13: 0.0015722253198696043 for jet 122 and jet 181\n",
    "```\n",
    "\n",
    "Then get a major difference here:\n",
    "\n",
    "```\n",
    "py: Iteration 88: 0.0131507280901848 for jet 322 and jet 323\n",
    "jl: Iteration 88: 0.012617123337897836 for jet 683 and jet -1\n",
    "```\n",
    "\n",
    "This makes me suspect there is a bug in the Julia metric calculation!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atell's Implementation\n",
    "\n",
    "#### AKA Julia Basic Implementation\n",
    "\n",
    "Get this working with our HepMC input file, so that we have performance numbers for a basic Julia version\n",
    "\n",
    "Use branch `graeme-chep` running a new `main()` from `chep.jl`\n",
    "\n",
    "- Reads events from HepMC3\n",
    "    - Using Philippe's hepmc3jl wrapper package\n",
    "        - N.B. to get this to work you have to start the Julia package manager and do `(JetReconstruction) pkg> develop ./hepmc3jl`\n",
    "- Events are read into PseudoJet objects, then converted to the Vector{Vector{Float64}} used by Atell's code\n",
    "- Final results are recorded as FinalJet objects (rap, phi, pt)\n",
    "- Added timing code that can wrap multiple runs and time the code properly\n",
    "- Added dump option to dump final jets as JSON\n",
    "    - This was used to check the results: Atell's code agrees with FastJet and all Python implenentations, confirming there is a small bug in Philippe's code somewhere\n",
    "\n",
    "Results:\n",
    "\n",
    "`julia --project=. ./chep.jl --maxevents=100 --nsamples=100 --gcoff ./test/data/events.hepmc3`\n",
    "\n",
    "- Windows: Time per event 840.6328400000003 ± 101.1319694550256 μs\n",
    "- WSL Ubuntu: Time per event 949.3078376000002 ± 166.92048348307372 μs\n",
    "\n",
    "N.B. This is the only code that actually runs faster on Windows; the jitter on WSL is higher that makes the GC still suspect?\n",
    "- Turning off GC well outside of the timing loop didn't make a difference\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared this with the FastJet `N2Plain` algorithm and it's 4-5x faster!\n",
    "- Maybe this is winning from the `@simd` markup? To be tested.\n",
    "\n",
    "Tested by commenting out the `@simd` optimisation and, i fact, makes almost no difference!\n",
    "\n",
    "Baseline @inbounds @simd:\n",
    "* Windows: Time per event 979.7908000000002 ± 187.1684725049544 μs\n",
    "\n",
    "Only @inbounds (no @simd)\n",
    "* Windows: Time per event 1006.6491699999999 ± 165.96200621872174 μs\n",
    "\n",
    "No optimisation macros (no @inbounds, no @simd)\n",
    "* Windows: Time per event 1137.9133900000002 ± 173.4552389619032 μs\n",
    "\n",
    "Only @simd (no @inbounds)\n",
    "* Windows: Time per event 1352.0714699999999 ± 212.99450383630403 μs\n",
    "\n",
    "Amazingly, @simd on its own makes things worse!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "antikt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
